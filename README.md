# Differtiable Tree Machine
This is the official repo accompanying the ICML 2023 publication [*Differentiable Tree Operations Promote Compositional
Generalization*](https://arxiv.org/abs/2306.00751).

## Setup
This repo has been tested with Python 3.8.13. You can install the necessary packages by running:
`pip install -r requirements.txt`. Some packages such as `pytorch` may need to be installed
via their own directions in order to install the correct version for your hardware (CPU or GPU).

## Data
Data available at https://huggingface.co/datasets/rfernand/basic_sentence_transforms. The primary four tasks used in the 
paper are: 

1. [car_cdr_seq](https://huggingface.co/datasets/rfernand/basic_sentence_transforms/resolve/main/car_cdr_seq.zip)
2. [active_logical_ttb](https://huggingface.co/datasets/rfernand/basic_sentence_transforms/resolve/main/active_logical_ttb.zip)
3. [passive_logical_ttb](https://huggingface.co/datasets/rfernand/basic_sentence_transforms/resolve/main/passive_logical_ttb.zip)
4. [actpass_logical_tt](https://huggingface.co/datasets/rfernand/basic_sentence_transforms/resolve/main/actpass_logical_tt.zip)

Each of these tasks should have a separate directory in `./data_files/`. For example, if you download
active_logical_ttb.zip, place the zip file in `./data_files/` and unzip it.
This should create the directory `data_files/active_logical_ttb/` which contains json files for the various data splits 
like train.jsonl, test.jsonl, etc. I recommend using [car_cdr_rcons.zip](https://huggingface.co/datasets/rfernand/basic_sentence_transforms/blob/main/car_cdr_rcons.zip) as a small task to get the code up and running.



## Primary Results
The following command can be used to train and evaluate the model used for the results in Table 1.

`python main.py --task_type=active_logical_ttb --dtm_steps=16 --max_tree_depth=12 --lr=1e-4 --steps=20000 --ctrl_hidden_dim=64
--transformer_norm_first=1 --wd=1e-1 --num_warmup_steps=10000 --gclip=1 --batch_size=16 --optim_beta2=.95 --train_log_freq=20 
--transformer_nheads=4 --scheduler=cosine --router_dropout=.1`

This command trains a model on the activeâ†”logical task. Change `--task_type` to one of the other tasks to train and evaluate
models for that specific task. All of the hyperparameters remain the same for each task, except for `--dtm_steps`. This should
be set according to each task:

```
car_cdr_seq: 12 steps
active_logical_ttb: 16 steps
passive_logical_ttb: 28 steps
actpass_logical_tt: 20 steps
```
## Learned Structural Transformation Ablation
To generate the values shown in Table 2, you can have the E and D matrices be learned by using the commandline argument
`--predefined_operations_are_random`.

## Gumbel Softmax ablations
To generate the values shown in Table 3, you can turn on Gumbel Softmax for argument selection by using the commandline
argument `--arg_dist_fn=gumbel`. Similarly, you can turn on Gumbel Softmax for operation selection by using the 
commandline argument `--op_dist_fn=gumbel`.



## wandb
You can set the $WANDB_API_KEY environment variable to use wandb.

## Citation
If you use this code, please cite the work:
```bibtex
@InProceedings{soulos23a, 
  title = {Differentiable Tree Operations Promote Compositional Generalization}, 
  author = {Soulos, Paul and Hu, Edward and McCurdy, Kate and Chen, Yunmo and Fernandez, Roland and Smolensky, Paul and Gao, Jianfeng}, 
  booktitle = {Proceedings of the 40th International Conference on Machine Learning}, 
  year = {2023}, 
  series = {Proceedings of Machine Learning Research}, 
  publisher = {PMLR} 
} 
```
